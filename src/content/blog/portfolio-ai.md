---
title: "Portfolio: Specialization - Fuzzy Brain-driven AI"
pubDate: 02-02-2025 18:00
author: "Jo Colomban"
tags:
  - Portfolio
  - UE5
imgUrl: "/assets/portfolio-ai/signalPriority.gif"
description: "An Unreal Engine 5 fuzzy-brain-driven AI"
layout: '../../layouts/BlogPost.astro'
---

## Introduction
GAMEPLAY VIDEO GOES HERE! <br>
This project was born as an improvement on the AI [David Mikulic](https://davidmikulic.com/) and I made during our last [game project]() during our education at FutureGames, with the goal of creating an AI system for a stealth game that can remember more than one thing, and act on those memories according to what's most important at the moment, trying to make it seem smarter than the average NPC guard :\).

## Fuzzy Brain basics
<img src='/assets/portfolio-ai/signalStruct.png' class='w-80% self-align-center'> </img>
The fulcrum of this AI is **UFuzzyBrainComponent**, an ActorComponent that can manages an array of **FWeightedSignals** and passes the currently most interesting one to the Blackboard of a Behavior Tree (from now on BT). This signals are, in this example, generated by two senses, **Hearing** and **Sight**, but it'd be trivial to add further signal generators without having to modify the *FuzzyBrain* itself.
Once a *WeightedSignal* signal is received, it gets added to the array that represents the AI's memory. Every tick, the memory is evaluated and the **currently most interesting signal** gets written to the Blackboard, so the BT can act on it. 
Every tick, all signals in memory decay by `PrejudiceDecay`, which is tweakable in editor, to make the AI give less importance to older signals.

<hr>

## Signal Severity and priority
<img src="/assets/portfolio-ai/severityThresholds.png" class="hidden" />
To allow a Behavior Tree to drive the AI's behavior based on the Fuzzy Brain's signals, a customizable set of **Signal Severities** has been defined. This allows to read the continuous **weight** of the most interesting signal as a more BT-friendly **discrete** value.

<div class="flex flex-col lg:flex-row items-center lg:space-x-4 space-y-4 lg:space-y-0 p-4">
    <div class="w-full lg:w-2/3">
        <img src="/assets/portfolio-ai/severityThresholds.png" class="rounded-lg" />
    </div>
    <div class="w-full lg:w-1/3">
        <h3>Severity thresholds</h3>
        <p class="text-justify">
            The severity <b>thresholds</b> can be defined by the designer in-editor in any <i>UFuzzyBrainComponent</i> as shown here. Those thresholds can then be used as conditional decorators to allow the BT to easily switch between behaviors according to the current severity.
        </p>
    </div>
</div>
<img src="/assets/portfolio-ai/signalThresholdBT.png" class="rounded-lg p-4" />

<div class="flex flex-col lg:flex-row items-center lg:space-x-4 space-y-4 lg:space-y-0 p-4">
    <div class="w-full lg:w-2/3">
        <img src="/assets/portfolio-ai/signalPriority.gif" class="rounded-lg" />
    </div>
    <div class="w-full lg:w-1/3">
        <h3>Signal priority</h3>
        <p class="text-justify">
            The big advantage of this Fuzzy Brain-driven AI is that it can store multiple signals in memory, while always focusing on the one with highest priority. In this example, the AI is storing the noise signals from the distractions the player is throwing past it, but it's still acting on the much **higher weight** vision signal that its getting from detecting the player in its narrow vision cone (see <a href="#sight">sight section below!</a>)
        </p>
    </div>
</div>

<div class="flex flex-col lg:flex-row items-center lg:space-x-4 space-y-4 lg:space-y-0 p-4">
    <div class="w-full lg:w-2/3">
        <img src="/assets/portfolio-ai/spiderEyes.gif" class="rounded-lg" />
    </div>
    <div class="w-full lg:w-1/3">
        <h3>Using the raw signal weight</h3>
        <p class="text-justify">
            While using severity thresholds ideal in most BT-related applications, the float signal weight of the current most interesting signal is available from <i>UFuzzyBrainComponent</i>'s BP API and it can be used, for example, for visual effects like this (very programmer-arty :D) glow on the spider's eyes that increases as the spider receives stronger and stronger signals, in this case from seeing the player. (Movement has been intentionally stopped while recording this GIF for clarity.)
        </p>
    </div>
</div>
<img src="/assets/portfolio-ai/spiderEyesAPI.png" class="rounded-lg p-4" />

<hr>

## Hearing
<img src='/assets/portfolio-ai/hearingThrowable.gif' class='hidden'></img>
*UHearingComponent* is one of the two custom senses made for this project, opting to build a solution from scratch rather than relying on UE's already existing sensing system to guarantee maximum customizability and full integration with the *FuzzyBrain* while keeping the code simple and lightweight.

<div class="flex flex-col lg:flex-row items-center lg:space-x-4 space-y-4 lg:space-y-0 p-4">
    <div class="w-full lg:w-2/3">
        <img src="/assets/portfolio-ai/hearingDiagram.png" class="rounded-lg" />
    </div>
    <div class="w-full lg:w-1/3">
        <h3>Code architecture</h3>
        <p class="text-justify">
            I tried to keep <b>noise makers</b> and <b>hearing components</b> as decoupled as possible. The GameMode creates a singleton of <i>UNoiseSystem</i>, and that will be the only thing everything hearing-related will talk to.
            <li> Things that make noise only have to implement the *INoiseMaker* interface and send a *UNoiseDataAsset* (see below) to *UNoiseSystem* whenever they want to make a noise.
            <li> Each <i>UHearingComponent</i> will register itself to <i>UNoiseSystem</i> at BeginPlay. <i>UNoiseSystem</i> will then <b>dispatch</b> every <b>noise event</b> to all registered <i>UHearingComponent</i>s <b>in the noise's range</b>
        </p>
    </div>
</div>

<div class="flex flex-col lg:flex-row items-center lg:space-x-4 space-y-4 lg:space-y-0 p-4">
    <div class="w-full lg:w-1/2 flex flex-col space-y-4">
        <img src="/assets/portfolio-ai/noiseDataAsset.png" class="rounded-lg" />
    </div>

   <div class="w-full lg:w-1/2 flex flex-col space-y-4 p-4">
        <div>
            <h3>Noise Events</h3>
        <p class="text-justify">
            To keep everything as designer-friendly as possible, everything that generates noise is defined by a <b>Noise Data Asset</b>. This allows in-engine tweaking of the sound's <b>radius</b> (represented by the purple wireframe in the GIFs), <b>intensity</b>, <b>distance-based falloff</b> and much more. A <b>sound effect</b> can also be passed in, and it'll be played by <i>UNoiseSystem</i> as a <b>3D SFX</b> at the correct location.
        </p>
        </div>
        <div>
            <img src="/assets/portfolio-ai/hearingThrowable.gif" class="rounded-lg" />
        </div>
    </div>
</div>

<hr>

## Sight
<img src='/assets/portfolio-ai/sightDemo2.gif' class='rounded-lg p-4' ></img>
*USightComponent* is one of the two custom senses made for this project, opting to build a solution from scratch rather than relying on UE's already existing sensing system to guarantee maximum customizability and full integration with the *FuzzyBrain* while keeping the code simple and lightweight.

<div class="flex flex-col lg:flex-row items-center lg:space-x-4 space-y-4 lg:space-y-0 p-4">
    <div class="w-full lg:w-1/2 flex flex-col space-y-4">
        <img src="/assets/portfolio-ai/sightBP.png" class="rounded-lg" />
    </div>

   <div class="w-full lg:w-1/2 flex flex-col space-y-4 p-4">
        <div>
        <h3>Sight component setup</h3>
        <p class="text-justify">
            The BP for the AI pawn can have <b>any amount</b> of <i>USightComponent</i>s. Each component can be tweaked through the parameters shown below, creating <b>multiple vision cones</b> with different <b>sensitivities</b>. In this example, the narrow vision cone detects things much quicker than the wider ones!
        </p>
        </div>
        <div>
            <img src="/assets/portfolio-ai/sightParams.png" class="rounded-lg" />
        </div>
    </div>
</div>

<hr>

## Waypoint system
<img src="/assets/portfolio-ai/Waypoints.gif" class="hidden" />

The AI, when the brain doesn't provide any signals interesting enough to investigate, will <b>default to a patrolling behavior</b>. To handle it in a <b>designer-friendly</b> way, I implemented a <b>waypoint system that is fully controllable in-editor</b>.

<div class="flex flex-col lg:flex-row items-center lg:space-x-4 space-y-4 lg:space-y-0 p-4">
    <div class="w-full lg:w-2/3">
        <img src="/assets/portfolio-ai/Waypoints.gif" class="rounded-lg" />
    </div>
    <div class="w-full lg:w-1/3">
        <h3>Waypoint handling in editor</h3>
        <p class="text-justify">
            By adding any amount of <b>UWaypointHolderComponent</b>s to any actor BP, the menu shown in the GIF will appear in every instance of that BP in a level. It allows the designer to set a number of waypoints for each <i>UWaypointHolderComponent</i> and a debug color. Once drawn, each waypoint will appear as a SceneComponent in the BP instance, allowing the designer to <b>move around their transforms in-engine</b>, allowing them to quickly modify the patrol path for the AI without having to touch any code or even the BT.
        </p>
    </div>
</div>

<div class="flex flex-col lg:flex-row items-center lg:space-x-4 space-y-4 lg:space-y-0 p-4">
    <div class="w-full lg:w-1/2 flex flex-col space-y-4">
        <img src="/assets/portfolio-ai/waypointsIngame.gif" class="rounded-lg" />
        <p class="text-justify">
           In case there is more than one set of waypoints, the API will return the correct one based on the brain's <b>current suspicion level</b>. This makes a BT sequence like the one show work for multiple suspicion levels!
        </p>
    </div>

   <div class="w-full lg:w-1/2 flex flex-col space-y-4 p-4">
        <div>
            <h3>In-game behavior</h3>
            <p class="text-justify">
                <i>UWaypointHolderComponent</i> exposes a <b>BP API</b> that allows transparent fetching of the next endpoint, in this case by a BT task.
            </p>
        </div>
        <div>
            <img src="/assets/portfolio-ai/waypointBT.png" class="rounded-lg" />
        </div>
    </div>
</div>

<hr>

## Credits
Special thanks to [David Mikulic](https://davidmikulic.com/entomon) for the procedurally animated spider the AI is driving, and [Rae Zeviar](https://www.artstation.com/artwork/2BlOVB) for the amazing victorian mansion modular kit I used for the first clip on this page! Check their portfolios out :\)



